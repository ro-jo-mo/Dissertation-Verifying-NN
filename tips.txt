options:
  -h, --help            show this help message and exit
  --config CONFIG       Path to YAML format config file.
  --device {cpu,cuda}   Select device to run verifier, cpu or cuda (GPU).
  --seed SEED           Random seed.
  --conv_mode {patches,matrix}
                        Convolution mode during bound propagation: "patches" mode (default) is very efficient, but may not support all architecture; "matrix" mode is slow but supports all architectures.
  --deterministic       Run code in CUDA deterministic mode, which has slower performance but better reproducibility.
  --double_fp           Use double precision floating point. GPUs with good double precision support are preferable (NVIDIA P100, V100, A100, H100; AMD Radeon Instinct MI50, MI100).
  --loss_reduction_func LOSS_REDUCTION_FUNC
                        When batch size is not 1, this reduction function is applied to reduce the bounds into a scalar (options are "sum" and "min").
  --no_sparse_alpha     Enable/disable sparse alpha.
  --no_sparse_interm    Enable/disable sparse intermediate bounds.
  --save_adv_example    Save returned adversarial example in file.
  --verify_onnxruntime_output
                        Check if the inference results of counterexample are the same on both PyTorch and ONNXRuntime.
  --eval_adv_example    Whether to validate the saved adversarial example.
  --show_adv_example    Print the adversarial example.
  --precompile_jit      Precompile jit kernels to speed up after jit-wrapped functions, but will cost extra time at the beginning.
  --prepare_only        Prepare to run the instance (e.g., cache vnnlib and converted onnx files) without running the actual verification.
  --complete_verifier {bab,mip,bab-refine,skip,auto,Customized}
                        Complete verification verifier. "bab": branch and bound with beta-CROWN or GCP-CROWN; "mip": mixed integer programming (MIP) formulation; "bab-refine": branch and bound with intermediate layer bounds
                        computed by MIP; "Customized": customized verifier, need to specially def by user.
  --no_incomplete       Enable/Disable initial alpha-CROWN incomplete verification (disable this can save GPU memory).
  --csv_name CSV_NAME   Name of .csv file containing a list of properties to verify (VNN-COMP specific).
  --results_file RESULTS_FILE
                        Path to results file.
  --root_path ROOT_PATH
                        Root path of the specification folder if using vnnlib.
  --deterministic_opt   To ensure that the returned alphas match the returned bounds, so that we can reproduce the bounds using the returned alphas during debugging.
  --graph_optimizer GRAPH_OPTIMIZER
                        BoundedModule model graph optimizer function name. For examples of customized graph optimizer, please see the config files for the gtrsb benchmark in VNN-COMP 2023.
  --buffer_has_batchdim
                        In most cases, the shape of buffers in an ONNX graph do not have a batch dimension. Enabling this option will help load models with a buffer object that has a batch dimension. In most case this can be
                        inferred in the verifier automatically, and this option is not needed.
  --save_output         Save output for test.
  --output_file OUTPUT_FILE
                        Path to the output file.
  --return_optimized_model
                        Return the model with optimized bounds after incomplete verification is done.
  --model MODEL         Model name. Will be evaluated as a python statement.
  --load_model LOAD_MODEL
                        Load pretrained model from this specified path.
  --onnx_path ONNX_PATH
                        Path to .onnx model file.
  --onnx_path_prefix ONNX_PATH_PREFIX
                        Add a prefix to .onnx model path to correct malformed csv files.
  --cache_onnx_conversion
                        Cache the model converted from ONNX.
  --debug_onnx          Debug onnx conversion.
  --onnx_quirks ONNX_QUIRKS
                        Load onnx model with quirks to workaround onnx model issue. This string will be passed to onnx2pytorch as the 'quirks' argument, and it is typically a literal of a python dict, e.g., "{'Reshape':
                        {'fix_batch_size: True'}}".
  --input_shape INPUT_SHAPE [INPUT_SHAPE ...]
                        Specified input shape of the model.Usually the shape can be automatically determined from dataset or onnx model, but some onnx models may have an incompatible shape (without batch dim). You can
                        specify shape explicitly here.The shape should be (-1, input_shape) like (-1, 3, 32, 32) and -1 indicates the batch dim.
  --onnx_loader ONNX_LOADER
                        ONNX model loader function name. Can be the Customized() primitive; for examples of customized model loaders, please see the config files for the marabou-cifar10 benchmark in VNN-COMP 2021 and the
                        Carvana benchmark in VNN-COMP 2022.
  --onnx_optimization_flags {merge_bn,merge_linear,merge_gemm,remove_ineffective_layers,remove_relu_in_last_layer} [{merge_bn,merge_linear,merge_gemm,remove_ineffective_layers,remove_relu_in_last_layer} ...]
                        Onnx graph optimization config.
  --onnx_vnnlib_joint_optimization_flags {peel_off_last_softmax_layer,none} [{peel_off_last_softmax_layer,none} ...]
                        Joint optimization that changes both onnx model and vnnlib.
  --check_optimized     Check the optimized onnx file instead the original one when converting to pytorch. This is used when input shape is changed during optimization.
  --flatten_final_output
                        Manually add a flatten layer at the end of the model.
  --optimize_graph OPTIMIZE_GRAPH
                        Specify a custom function for optimizing the graph on the BoundedModule.
  --model_with_jacobian
                        Indicate that the model contains JacobianOP.
  --start START         Start from the i-th property in specified dataset.
  --end END             End with the (i-1)-th property in the dataset.
  --select_instance SELECT_INSTANCE [SELECT_INSTANCE ...]
                        Select a list of instances to verify.
  --num_outputs NUM_OUTPUTS
                        Number of classes for classification problem.
  --mean MEAN [MEAN ...]
                        Mean vector used in data preprocessing.
  --std STD [STD ...]   Std vector used in data preprocessing.
  --pkl_path PKL_PATH   Load properties to verify from a .pkl file (only used for oval20 dataset).
  --dataset DATASET     Dataset name (only if not using specifications from a .csv file). Dataset must be defined in utils.py. For customized data, checkout custom/custom_model_data.py.
  --filter_path FILTER_PATH
                        A filter in pkl format contains examples that will be skipped (not used).
  --data_idx_file DATA_IDX_FILE
                        A text file with a list of example IDs to run.
  --spec_type {lp,box,bound}
                        Type of verification specification. "lp" = L_p norm, "box" = element-wise lower and upper bound provided by dataloader.
  --robustness_type {verified-acc,runnerup,clean-acc,specify-target,all-positive}
                        For robustness verification: verify against all labels ("verified-acc" mode), or just the runnerup labels ("runnerup" mode), or using a specified label in dataset ("specify-target" mode, only used for
                        oval20). Not used when a VNNLIB spec is used.
  --norm NORM           Lp-norm for epsilon perturbation in robustness verification (1, 2, inf).
  --epsilon EPSILON     Set perturbation size (Lp norm). If not set, a default value may be used based on dataset loader.
  --epsilon_min EPSILON_MIN
                        Set an optional minimum perturbation size (Lp norm).
  --vnnlib_path VNNLIB_PATH
                        Path to .vnnlib specification file. Will override any Lp/robustness verification arguments.
  --vnnlib_path_prefix VNNLIB_PATH_PREFIX
                        Add a prefix to .vnnlib specs path to correct malformed csv files.
  --rhs_offset RHS_OFFSET
                        Adding an offset to RHS.
  --batch_size BATCH_SIZE
                        Batch size in bound solver (number of parallel splits).
  --auto_enlarge_batch_size
                        Automatically increase batch size based on --batch_size in bab if current VRAM usage < 45%, only support input_split.
  --min_batch_size_ratio MIN_BATCH_SIZE_RATIO
                        The minimum batch size ratio in each iteration (splitting multiple layers if the number of domains is smaller than min_batch_size_ratio * batch_size).
  --use_float64_in_last_iteration
                        Use double fp (float64) at the last iteration in alpha/beta CROWN.
  --early_stop_patience EARLY_STOP_PATIENCE
                        Number of iterations that we will start considering early stop if tracking no improvement.
  --start_save_best START_SAVE_BEST
                        Start to save best optimized bounds when i > int(iteration*start_save_best). Early iterations are skipped for better efficiency.
  --bound_prop_method {alpha-crown,crown,forward,forward+crown,alpha-forward,crown-ibp,init-crown,ibp,dynamic-forward,dynamic-forward+crown,dynamic-forward+backward}
                        Bound propagation method used for incomplete verification and input split based branch and bound.
  --init_bound_prop_method {same,alpha-crown,crown,forward,forward+crown,alpha-forward,crown-ibp,init-crown}
                        Bound propagation method used for the initial bound in input split based branch and bound. If "same" is specified, then it will use the same method as "bound_prop_method".
  --prune_after_crown   After CROWN pass, prune verified labels before starting the alpha-CROWN pass.
  --optimize_disjuncts_separately
                        If set, each neuron computes separate bounds for each disjunct. If set, do not set prune_after_crown=True.
  --crown_batch_size CROWN_BATCH_SIZE
                        Batch size in batched CROWN.
  --max_crown_size MAX_CROWN_SIZE
                        Max output size in CROWN (when there are too many output neurons, only part of them will be bounded by CROWN).
  --activation_bound_option {adaptive,same-slope,zero-lb,one-lb}
                        Options for specifying the the way to initialize CROWN bounds for acvitaions.
  --compare_crown_with_ibp
                        Compare CROWN bounds with IBP bounds given existing intermediate bounds.
  --no_alpha            Disable/Enable alpha crown.
  --lr_init_alpha LR_INIT_ALPHA
                        Learning rate for the optimizable parameter alpha in alpha-CROWN bound.
  --init_iteration INIT_ITERATION
                        Number of iterations for alpha-CROWN incomplete verifier.
  --share_alphas        Share some alpha variables to save memory at the cost of slightly looser bounds.
  --alpha_lr_decay ALPHA_LR_DECAY
                        Learning rate decay factor during alpha-CROWN optimization. Need to use a larger value like 0.99 or 0.995 when you increase the number of iterations.
  --no_full_conv_alpha  Enable/disable the use of independent alpha for conv layers.
  --max_coeff_mul MAX_COEFF_MUL
                        Maximum coefficient value in the optimizable parameters for BoundMul.
  --matmul_share_alphas
                        Check alpha sharing for matmul.
  --disable_optimization DISABLE_OPTIMIZATION [DISABLE_OPTIMIZATION ...]
                        A list of the names of operators which have bound optimization disabled.
  --init_max_time INIT_MAX_TIME
                        Maximum time for the initial bound optimization (relative to the total timeout).
  --apply_output_constraints_to [APPLY_OUTPUT_CONSTRAINTS_TO ...]
                        Includes the output constraint in the optimization of linear layers. Can be a comma separated list of layer types (e.g. "BoundLinear"), layer names (e.g. "/input.7") or "all". This will disable patch
                        mode for listed conv layers. When set, --optimize_disjuncts_separately must be set, too, if the safety property uses a disjunction.
  --tighten_input_bounds
                        Tighten input bounds using output constraints. If set, --apply_output_constraints_to should contain "BoundInput" or the corresponding layer name.
  --best_of_oc_and_no_oc
                        Computes bounds of each layer both with and without output constraints. The best of both is used. Increases the runtime, but may improve the bounds.
  --directly_optimize [DIRECTLY_OPTIMIZE ...]
                        A list of layer names whose bounds should be directly optimized for. IBP is disabled for these layers. Should only be needed for backward verification (approximation of input bounds) where the first
                        linear layer defines the cs and should be optimized.
  --oc_lr OC_LR         The learning rate for the dualized output constraints.
  --share_gammas        Shares gammas across neurons in the optimized layer.
  --lr_alpha LR_ALPHA   Learning rate for optimizing alpha during branch and bound.
  --lr_beta LR_BETA     Learning rate for optimizing beta during branch and bound.
  --lr_decay LR_DECAY   Learning rate decay factor during beta-CROWN optimization. Need to use a larger value like 0.99 or 0.995 when you increase the number of iterations.
  --optimizer OPTIMIZER
                        Optimizer used for alpha and beta optimization.
  --iteration ITERATION
                        Number of iteration for optimizing alpha and beta during branch and bound.
  --no_beta             Enable/disable beta split constraint (this option is for ablation study only and should not be used normally).
  --no_beta_warmup      Enable/disable beta warmup from branching history (this option is for ablation study only and should not be used normally).
  --enable_opt_interm_bounds
                        Enable optimizing intermediate bounds for beta-CROWN, only used when mip refine for now.
  --enable_all_node_split_LP
                        When all nodes are split during Bab but not verified, using LP to check.
  --forward_refine      Refine forward bound with CROWN for unstable neurons.
  --forward_max_dim FORWARD_MAX_DIM
                        Maximum input dimension for forward bounds in a batch.
  --reset_start_layer_threshold RESET_START_LAYER_THRESHOLD
                        Reset the start layer if timeout neurons are above the threshold.
  --label_batch_size LABEL_BATCH_SIZE
                        Maximum target labels to handle in alpha-CROWN. Cannot be too large due to GPU memory limit.
  --no_skip_with_refined_bound
                        By default we skip the second alpha-CROWN execution if all alphas are already initialized. Setting this to avoid this feature.
  --mip_unstable_neuron_threshold MIP_UNSTABLE_NEURON_THRESHOLD
                        When complete_verifier=auto, enable MIP refinement when the number of unstable neurons exceeds this threshold.
  --mip_multi_proc MIP_MULTI_PROC
                        Number of multi-processes for mip solver. Each process computes a mip bound for an intermediate neuron. Default (None) is to auto detect the number of CPU cores (note that each process may use
                        multiple threads, see the next option).
  --mip_threads MIP_THREADS
                        Number of threads for echo mip solver process (default is to use 1 thread for each solver process).
  --mip_perneuron_refine_timeout MIP_PERNEURON_REFINE_TIMEOUT
                        MIP timeout threshold for improving each intermediate layer bound (in seconds).
  --mip_refine_timeout MIP_REFINE_TIMEOUT
                        Percentage (x100%) of time used for improving all intermediate layer bounds using mip. Default to be 0.8*timeout.
  --no_mip_early_stop   Enable/disable early stop when finding a positive lower bound or a adversarial example during MIP.
  --disable_adv_warmup  Disable using PGD adv as MIP refinement warmup starts.
  --mip_solver {gurobi,scip}
                        MLP/LP solver package (SCIP support is experimental).
  --mip_skip_unsafe     Do not use MIP solver to find counterexamples. This is useful in the case the MIP formulation is a relaxation and not complete.
  --topk_percentage TOPK_PERCENTAGE
                        Refine top k neurons in mip solver.
  --mip_refine_sliding_window MIP_REFINE_SLIDING_WINDOW
                        Only when the neural network is very deep or too many timeout neurons that we use it to simplify the problem.
  --mip_refine_extra_input_constraints
                        Add extra contraints to the MIP solver between the input and the starting layer, only activate when sliding window is enabled.
  --mip_perneuron_refine_timeout_increasement MIP_PERNEURON_REFINE_TIMEOUT_INCREASEMENT
                        MIP timeout threshold increasment for improving each intermediate layer bound (in seconds) if the remaining time is sufficient.
  --mip_timeout_neuron MIP_TIMEOUT_NEURON
                        Threshold for the timeout neurons to reset the perneuron timeout.
  --mip_remaining_timeout_coefficient_for_increasement MIP_REMAINING_TIMEOUT_COEFFICIENT_FOR_INCREASEMENT
                        MIP remaining timeout multiplier to determine whether following layer should increase the perneuron timeout.
  --mip_refine_remove_neurons
                        Remove unstable neurons from MIP based on A_dict from compute_bounds.
  --create_lp_model     Create linear programming model instead of MIP.
  --initial_max_domains INITIAL_MAX_DOMAINS
                        Number of domains we can add to domain list at the same time before bab. For multi-class problems this can be as large as the number of labels.
  --backing_up_max_domain BACKING_UP_MAX_DOMAIN
                        Backup value for the number of domains. This will be set to the same value as initial_max_domains.
  --max_domains MAX_DOMAINS
                        Max number of subproblems in branch and bound.
  --decision_thresh DECISION_THRESH
                        Decision threshold of lower bounds. When lower bounds are greater than this value, verification is successful. Set to 0 for robustness verification.
  --timeout TIMEOUT     Timeout (in second) for verifying one image/property.
  --timeout_scale TIMEOUT_SCALE
                        Scale the timeout for development purpose.
  --max_iterations MAX_ITERATIONS
                        Maximum number of BaB iterations.
  --override_timeout OVERRIDE_TIMEOUT
                        Override timeout.
  --get_upper_bound     Update global upper bound during BaB (has extra overhead, typically the upper bound is not used).
  --disable_pruning_in_iteration
                        Disable verified domain pruning within iteration.
  --pruning_in_iteration_ratio PRUNING_IN_ITERATION_RATIO
                        When ratio of positive domains >= this ratio, pruning in iteration optimization is open.
  --sort_targets        Sort targets before BaB.
  --disable_batched_domain_list
                        Disable batched domain list. Batched domain list is faster but picks domain to split in an unsorted way.
  --optimized_interm OPTIMIZED_INTERM
                        A list of layer names that will be optimized during branch and bound, separated by comma.
  --no_interm_transfer  Skip the intermediate bound transfer to save transfer-to-CPU time. Require intermediate bound does not change. Caution: cannot be used with cplex cut or intermediate beta refinement.
  --recompute_interm    Recompute all the intermediate bounds during BaB.
  --sort_domain_interval SORT_DOMAIN_INTERVAL
                        If unsorted domains are used, sort the domains every sort_domain_interval iterations.
  --vanilla_crown_bab   Use vanilla CROWN during BaB.
  --tree_traversal {depth_first,breadth_first}
                        During BaB, unkown domains can continue being split (deepening the tree, i.e. depth first traversal) or split only when all other unknown domains have the same number of splits (keeping the tree
                        shallow for as long as possible, i.e. breadth first traversal). Depth first traversal minimizes memory access time, breadth first traversal is beneficial for BICCOS.
  --enable_cut          Enable cutting planes using GCP-CROWN.
  --cuts_path CUTS_PATH
                        For cuts from CPLEX, specify the path for saving intermediate files with generated cuts.
  --enable_implication  Enable neuron implications.
  --enable_bab_cut      Enable cut constraints optimization during BaB.
  --enable_lp_cut       enable lp with cut constraints to debug
  --cut_method CUT_METHOD
                        Cutting plane generation method (unused, for future extensions).
  --lr_cuts LR_CUTS     Learning rate for optimizing cuts.
  --cut_lr_decay CUT_LR_DECAY
                        Learning rate decay for optimizing betas in GCP-CROWN.
  --cut_iteration CUT_ITERATION
                        Iterations for optimizing betas in GCP-CROWN.
  --cut_bab_iteration CUT_BAB_ITERATION
                        Iterations for optimizing betas in GCP-CROWN during branch and bound. Set to -1 to use the same number of iterations without cuts.
  --cut_early_stop_patience CUT_EARLY_STOP_PATIENCE
                        Early stop patience for optimizing cuts. Set to -1 to use the same value when cuts are not used.
  --cut_lr_beta CUT_LR_BETA
                        Learning rate for optimizing betas in GCP-CROWN.
  --number_cuts NUMBER_CUTS
                        Maximum number of cuts that we want to add.
  --topk_implication TOPK_IMPLICATION
                        Only keep top K constraints when filtering cuts.
  --batch_size_primal BATCH_SIZE_PRIMAL
                        Batch size when calculate primals, should be negative correlated to number of unstable neurons.
  --cut_max_num CUT_MAX_NUM
                        Maximum number of cuts.
  --enable_patches_cut  Enable GCP-CROWN optimization for intermediate layer bounds in patches mode.
  --cplex_cuts          Build and save mip mps models, let cplex find cuts, and use found cuts to improve lower bounds.
  --cplex_cuts_wait CPLEX_CUTS_WAIT
                        Wait a bit after cplex warmup in seconds, so that we tend to get some cuts at early stage of branch and bound.
  --no_cplex_cuts_revpickup
                        Enable/disable the inverse order domain pickout when cplex is enabled.
  --no_cut_reference_bounds
                        Enable/disable using reference bounds when GCP-CROWN cuts are used.
  --fix_cut_intermediate_bounds
                        Fix intermediate bounds when GCP-CROWN cuts are used.
  --biccos_cuts         BICCOS from the the Branch and Bound process.
  --no_biccos_constraint_strengthening
                        Do not use constraint strengthening in BICCOS.
  --biccos_recursively_strengthening
                        Use recursive constraint strengthening.
  --biccos_drop_ratio BICCOS_DROP_RATIO
                        Neuron drop ratio parameter when using inferred cut and neuron influence score heuristic.
  --biccos_verified_bonus BICCOS_VERIFIED_BONUS
                        When a neuron is verified, we add a bonus to the neuron influence score.
  --biccos_max_infer_iter BICCOS_MAX_INFER_ITER
                        After max_infer_iter iterations, we will stop inferencing cuts.
  --biccos_dropping_heuristics {neuron_influence_score,random_drop,sparse_opt}
                        Neuron dropping heuristic.
  --biccos_save_biccos_cuts
                        Save cuts to log/biccos.txt.
  --multi_tree_branching
                        Enables multi-tree branching. Instead of picking one new split per unkown domain, multiple splits are tested. In the end, the best (measured in bound tightness) regular BaB tree is selected.
  --multi_tree_branching_restore_best_tree
                        Restore the best tree after the multi-tree search.
  --multi_tree_branching_keep_n_best_domains MULTI_TREE_BRANCHING_KEEP_N_BEST_DOMAINS
                        Number of best domains to keep in multi-tree branching.
  --multi_tree_branching_k_splits MULTI_TREE_BRANCHING_K_SPLITS
                        Number of splits for each neuron in multi-tree branching.
  --multi_tree_branching_iterations MULTI_TREE_BRANCHING_ITERATIONS
                        Number of iterations for multi-tree branching.
  --branching_method {babsr,fsb,kfsb,kfsb-intercept-only,random,intercept,naive,sb,nonlinear,brute-force}
                        Branching heuristic. babsr is fast but less accurate; fsb is slow but most accurate; kfsb is usually a balance; kfsb-intercept-only is faster but may lead to worse branching; sb is fast smart
                        branching which relies on the A matrix.
  --branching_candidates BRANCHING_CANDIDATES
                        Number of candidates to consider when using fsb or kfsb. More candidates lead to slower but better branching.
  --branching_reduceop {min,max,mean,auto}
                        Reduction operation to compute branching scores from two sides of a branch (min or max). max can work better on some models.
  --enable_intermediate_bound_opt
                        Enable optimizing intermediate bounds for during bab.
  --branching_input_and_activation
                        Branching input domains and relu domains (experimental).
  --branching_input_and_activation_order BRANCHING_INPUT_AND_ACTIVATION_ORDER [BRANCHING_INPUT_AND_ACTIVATION_ORDER ...]
                        Order of branching input domains and relu domains (experimental).
  --branching_input_iterations BRANCHING_INPUT_ITERATIONS
                        Number of iterations to run input split before we run relu split.
  --branching_relu_iterations BRANCHING_RELU_ITERATIONS
                        Number of iterations to run relu split before we run input split.
  --nonlinear_split_method {bbps,babsr-like}
                        Branching heuristic for the general nonlinear functions (either the default BBPS heuristic, or a BaBSR-like as a baseline).
  --disable_nonlinear_split
                        Disable GenBaB even if there are non-ReLU functions to split.
  --branching_point_method {uniform,opt}
                        For general nonlinear functions, the method for choosing the branching point ("uniform" stands for branching in the middle while "opt" stands for using pre-optimized branching points).
  --nonlinear_branches NONLINEAR_BRANCHES
                        Number of branches for nonlinear branching.
  --nonlinear_split_filtering
                        KFSB-like filtering in general nonlinear branching.
  --nonlinear_split_filtering_beta
                        Use beta in the KFSB-like filtering.
  --nonlinear_split_filtering_clamp
                        Clamping scores to 0 in BBPS filtering.
  --filter_batch_size FILTER_BATCH_SIZE
                        Batch size for filtering.
  --filter_iterations FILTER_ITERATIONS
                        Number of iterations for filtering.
  --nonlinear_split_relu_only
                        When using BBPS, only consider branching ReLU instead of all the nonlinearities.
  --loose_tanh_threshold LOOSE_TANH_THRESHOLD
                        Set a threshold for tanh/sigmoid to use a different relaxation when the pre-activation bounds are too loose.
  --branching_point_db BRANCHING_POINT_DB
                        Path to store pre-computed branching points.
  --branching_point_num_iterations BRANCHING_POINT_NUM_ITERATIONS
                        Number of iterations for the optimization.
  --branching_point_batch_size BRANCHING_POINT_BATCH_SIZE
                        Batch size for the optimization.
  --branching_point_range_l BRANCHING_POINT_RANGE_L
                        Range (lower bound) for the optimization.
  --branching_point_range_u BRANCHING_POINT_RANGE_U
                        Range (upper bound) for the optimization.
  --branching_point_log_interval BRANCHING_POINT_LOG_INTERVAL
                        Log interval for the optimization.
  --branching_point_step_size_1d BRANCHING_POINT_STEP_SIZE_1D
                        Step size for 1d nonlinearities.
  --branching_point_step_size BRANCHING_POINT_STEP_SIZE
                        Step size for 2d and above nonlinearities.
  --enable_clip_domains
                        Shrinks subdomains based on their specification.
  --split_hint SPLIT_HINT [SPLIT_HINT ...]
                        Specifies value to split at.
  --reorder_bab         Uses a reordered implementation of the input BaB procedure that bounds, splits and clips domains rather than splitting, bounding, and clipping domains.
  --enable_input_split  Branch on input domain rather than unstable neurons.
  --enhanced_bound_prop_method {alpha-crown,crown,forward+crown,crown-ibp}
                        Specify a tighter bound propagation method if a problem cannot be verified after --input_split_enhanced_bound_patience.
  --enhanced_branching_method {sb,naive}
                        Specify a branching method if a problem cannot be verified after --input_split_enhanced_bound_patience.
  --input_split_enhanced_bound_patience INPUT_SPLIT_ENHANCED_BOUND_PATIENCE
                        Time in seconds that will use an enhanced bound propagation method (e.g., alpha-CROWN) to bound input split sub domains.
  --input_split_attack_patience INPUT_SPLIT_ATTACK_PATIENCE
                        Time in seconds that will start PGD attack to find adv examples during input split.
  --input_split_adv_check INPUT_SPLIT_ADV_CHECK
                        After the number of visited nodes, we will run adv_check in input split.
  --input_split_partitions INPUT_SPLIT_PARTITIONS
                        How many domains to split to for each dimension at each time. By default, it is 2. In very few limited experimental cases, can change to larger numbers.
  --sb_margin_weight SB_MARGIN_WEIGHT
                        Weight for the margin term in the sb heuristic.
  --sb_sum              Use sum for multiple specs in sb.
  --sb_primary_spec SB_PRIMARY_SPEC
                        Focus on one particular spec for the SB score.
  --bf_backup_thresh BF_BACKUP_THRESH
                        Threshold for using the SB score as the backup when the brute force score is too bad.
  --bf_rhs_offset BF_RHS_OFFSET
                        An offset on RHS used in computing the brute force heuristic.
  --bf_iters BF_ITERS   Number of iterations to use brute force.
  --bf_batch_size BF_BATCH_SIZE
                        A special batch size for brute force.
  --bf_zero_crossing_score
                        A zero crossing score in BF.
  --touch_zero_score TOUCH_ZERO_SCORE
                        A touch-zero score in BF.
  --ibp_enhancement     Use IBP bounds to enhance.
  --compare_input_split_with_old_bounds
                        Compare bounds after an input split with bounds before the split and take the better one.
  --input_split_update_rhs_with_attack
                        Run attack during input split and update RHS. BaB does not stop even if any counterexample is found.
  --sb_coeff_thresh SB_COEFF_THRESH
                        Clamp values of coefficient matrix (A matrix) for sb branching heuristic.
  --input_split_sort_index INPUT_SPLIT_SORT_INDEX
                        The output index to use for sorting domains in the input split.
  --no_input_split_sort_descending
                        Sort input split domains in an ascending/descending way.
  --input_split_show_progress
                        Show progress during input split.
  --input_split_presplit_domains INPUT_SPLIT_PRESPLIT_DOMAINS
                        Load pre-split domains from a file.
  --input_split_skip_getting_worst_domain
                        Skip getting the worst domain at the end of each iteration, to save some time cost when the domain list is long.
  --enable_bab_attack   Enable beam search based BaB-attack.
  --beam_candidates_number BEAM_CANDIDATES_NUMBER
                        Number of candidates in beam search.
  --beam_split_depth BEAM_SPLIT_DEPTH
                        Max additional level of splits to expand during beam search in BaB-Attack.
  --max_dive_fix_ratio MAX_DIVE_FIX_RATIO
                        Maximum ratio of fixed neurons during diving in BaB-Attack.
  --min_local_free_ratio MIN_LOCAL_FREE_RATIO
                        Minimum ratio of free neurons during local search in BaB-Attack.
  --submip_start_iteration SUBMIP_START_ITERATION
                        Iteration number to start sub-MIPs in BaB-Attack.
  --submip_timeout SUBMIP_TIMEOUT
                        Sub-MIP timeout threshold.
  --adv_pool_threshold ADV_POOL_THRESHOLD
                        Minimum value of difference when adding to adv_pool; default `None` means auto select.
  --refined_mip_attacker
                        Use full alpha crown bounds to refined intermediate bounds for sub-MIPs.
  --refined_batch_size REFINED_BATCH_SIZE
                        Batch size for full alpha-CROWN to refined intermediate bounds for mip solver attack (to avoid OOM), default None to be the same as mip_multi_proc.
  --pgd_order {before,after,middle,input_bab,skip}
                        Run PGD attack before/after/during incomplete verification, only during input bab, or skip it.
  --pgd_steps PGD_STEPS
                        Steps of PGD attack.
  --pgd_restarts PGD_RESTARTS
                        Number of random PGD restarts.
  --pgd_batch_size PGD_BATCH_SIZE
                        Batch size for number of restarts in PGD.
  --no_pgd_early_stop   Enable/disable early stop PGD when an adversarial example is found.
  --pgd_lr_decay PGD_LR_DECAY
                        Learning rate decay factor used in PGD attack.
  --pgd_alpha PGD_ALPHA
                        Step size of PGD attack. Default (auto) is epsilon/4.
  --pgd_alpha_scale     Scale PGD alpha according to data_max-data_min.
  --pgd_loss_mode {hinge,sum}
                        Loss mode for choosing the best delta.
  --pgd_restart_when_stuck
                        Restart adversarial noise when they do not change over attack iterations.
  --mip_attack          Use MIP (Gurobi) based attack if PGD cannot find a successful adversarial example.
  --adv_saver ADV_SAVER
                        Customized saver of adverserial examples.
  --adv_verifier ADV_VERIFIER
                        Customized verifier of adverserial examples.
  --early_stop_condition EARLY_STOP_CONDITION
                        Customized early stop condition.
  --adv_example_finalizer ADV_EXAMPLE_FINALIZER
                        Customized generation of adversarial examples, margins computation, etc.
  --pgd_loss PGD_LOSS   Customized pgd loss.
  --cex_path CEX_PATH   Save path for counter-examples.
  --attack_mode {diversed_PGD,diversed_GAMA_PGD,PGD,boundary}
                        Attack algorithm, including vanilla PGD and PGD with diversified output (Tashiro et al.), and GAMA loss (Sriramanan et al.).
  --attack_tolerance ATTACK_TOLERANCE
                        Tolerance of floating point error when checking whether attack is successful or not.
  --attack_func ATTACK_FUNC
                        The specific customized attack.
  --attack_gama_lambda ATTACK_GAMA_LAMBDA
                        Regularization parameter in GAMA attack.
  --attack_gama_decay ATTACK_GAMA_DECAY
                        Decay of regularization parameter in GAMA attack.
  --check_clean         Check clean prediction before attack.
  --input_split_pgd_steps INPUT_SPLIT_PGD_STEPS
                        Steps of PGD attack in input split before branching starts.
  --input_split_pgd_restarts INPUT_SPLIT_PGD_RESTARTS
                        Number of random PGD restarts in input split before branching starts.
  --input_split_pgd_alpha INPUT_SPLIT_PGD_ALPHA
                        Step size (alpha) in input split before branching starts.
  --input_split_enhanced_pgd_steps INPUT_SPLIT_ENHANCED_PGD_STEPS
                        Steps of PGD attack in enhanced pgd attack in input split.
  --input_split_enhanced_pgd_restarts INPUT_SPLIT_ENHANCED_PGD_RESTARTS
                        Number of random PGD restarts in enhanced pgd attack in input split.
  --input_split_enhanced_pgd_alpha INPUT_SPLIT_ENHANCED_PGD_ALPHA
                        Step size (alpha) in enhanced pgd attack in input split.
  --enable_check_adv {auto,true,false}
                        Enable or disable counterexample checking during input-space branch-and-bound. Default is 'auto', which disables check_adv when pgd_order is skip.
  --input_split_check_adv_pgd_steps INPUT_SPLIT_CHECK_ADV_PGD_STEPS
                        Steps of PGD attack in input split after each branching.
  --input_split_check_adv_pgd_restarts INPUT_SPLIT_CHECK_ADV_PGD_RESTARTS
                        Number of random PGD restarts in input split after each branching.
  --input_split_check_adv_pgd_alpha INPUT_SPLIT_CHECK_ADV_PGD_ALPHA
                        Step size (alpha) in input split after each branching.
  --input_split_check_adv_max_num_domains INPUT_SPLIT_CHECK_ADV_MAX_NUM_DOMAINS
                        Maximum number of domains for running attack during input split.
  --view_model          Print more detailed model information for analysis.
  --lp_test {MIP,LP,None}
                        Debugging option. Do not use.
  --rescale_vnnlib_ptb RESCALE_VNNLIB_PTB
                        Rescale the perturbation on X in vnnlib files to make them smaller. Used for debugging.
  --test_optimized_bounds
                        Debugging option. Compares optimized bounds with those from Gurobi for the relaxed LP formulation
  --test_optimized_bounds_after_n_iterations TEST_OPTIMIZED_BOUNDS_AFTER_N_ITERATIONS
                        Debugging option. Controls how many optimization steps for all layers are done before bounds are checked against the LP solver
  --print_verbose_decisions
                        Print more detailed information about branching decisions
  --sanity_check {Full,Full+Graph,Worst,None}
                        Using pgd upper bound as rhs-offset to check the feasibility of the method. Warning: If rhs_offset was specified in the config or command line, it will get ignored.
  --save_minimal_config SAVE_MINIMAL_CONFIG
                        Path to save a minimal config file.
  --save_minimal_config_omit_keys SAVE_MINIMAL_CONFIG_OMIT_KEYS [SAVE_MINIMAL_CONFIG_OMIT_KEYS ...]
                        Keys to omit from the minimal config file.